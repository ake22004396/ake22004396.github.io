{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14565026,"sourceType":"datasetVersion","datasetId":9303328},{"sourceId":14714031,"sourceType":"datasetVersion","datasetId":9400865}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nAuthor: ChatGPT\nIdea:\n1. ä» Kaggle Input åŠ è½½ universal_innovation_dataset.jsonl\n2. ä½¿ç”¨ BGE ä¸­æ–‡å‘é‡æ¨¡å‹ç¼–ç  task\n3. æ„å»º FAISS ä½™å¼¦ç›¸ä¼¼åº¦ç´¢å¼•ï¼ˆIndexFlatIPï¼‰\n4. ä¿å­˜ç´¢å¼•ä¸å…ƒæ•°æ®åˆ° /kaggle/working\n5. æä¾›åŸºç¡€æ£€ç´¢æ¥å£ï¼ˆéäº¤äº’ï¼Œé€‚åˆ Kaggleï¼‰\n\"\"\"\n\nimport os\nimport json\nimport numpy as np\nimport faiss\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ============================================================\n# Kaggle è·¯å¾„é…ç½®ï¼ˆâš ï¸ Input åªè¯»ï¼ŒOutput å†™ workingï¼‰\n# ============================================================\nJSONL_PATH = \"/kaggle/input/universal-innovation-training-set/kaggle_upload/universal_innovation_dataset.jsonl\"\n\nINDEX_PATH = \"/kaggle/working/innovation_faiss.index\"\nMETADATA_PATH = \"/kaggle/working/innovation_metadata.json\"\n\n# ============================================================\n# 1. åŠ è½½ JSONL æ•°æ®\n# ============================================================\ndef load_jsonl(filepath):\n    texts = []\n    metadata = []\n\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}\")\n\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        for line_num, line in enumerate(f, 1):\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                record = json.loads(line)\n                task = record.get(\"task\") or record.get(\"query_text\")\n                if not task:\n                    continue\n                texts.append(task)\n                metadata.append(record)\n            except Exception:\n                continue\n\n    print(f\"âœ… æˆåŠŸåŠ è½½ {len(texts):,} æ¡æœ‰æ•ˆæ ·æœ¬\")\n    return texts, metadata\n\n\n# ============================================================\n# 2. æ„å»º FAISS ç´¢å¼•\n# ============================================================\ndef build_faiss_index(texts, index_path):\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}\")\n\n    print(\"ğŸ“¥ åŠ è½½ BGE ä¸­æ–‡å‘é‡æ¨¡å‹...\")\n    model = SentenceTransformer(\"BAAI/bge-large-zh-v1.5\", device=device)\n\n    print(f\"ğŸ”„ ç¼–ç æ–‡æœ¬å‘é‡ï¼ˆå…± {len(texts):,} æ¡ï¼‰...\")\n    vectors = model.encode(\n        texts,\n        batch_size=32,\n        normalize_embeddings=True,\n        show_progress_bar=True,\n    )\n\n    vectors = np.asarray(vectors, dtype=\"float32\")\n    dim = vectors.shape[1]\n\n    print(f\"ğŸ“Š å‘é‡ç»´åº¦: {dim}\")\n    index = faiss.IndexFlatIP(dim)\n    index.add(vectors)\n\n    print(f\"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {index.ntotal:,} æ¡\")\n\n    faiss.write_index(index, index_path)\n    print(f\"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜è‡³: {index_path}\")\n\n    return index, model\n\n\n# ============================================================\n# 3. ä¿å­˜ / åŠ è½½å…ƒæ•°æ®\n# ============================================================\ndef save_metadata(metadata, filepath):\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        json.dump(metadata, f, ensure_ascii=False)\n    print(f\"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜è‡³: {filepath}\")\n\n\ndef load_metadata(filepath):\n    if not os.path.exists(filepath):\n        return None\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\n\n# ============================================================\n# 4. ç¤ºä¾‹æ£€ç´¢å‡½æ•°ï¼ˆé€‚åˆ Kaggleï¼Œä¸ç”¨ inputï¼‰\n# ============================================================\ndef search_demo(model, index, metadata, query, top_k=3):\n    query_vec = model.encode(\n        [query],\n        normalize_embeddings=True\n    ).astype(\"float32\")\n\n    D, I = index.search(query_vec, top_k)\n\n    print(f\"\\nğŸ” Query: {query}\")\n    for rank, (idx, score) in enumerate(zip(I[0], D[0]), 1):\n        item = metadata[idx]\n        print(f\"\\n--- Top {rank} | ç›¸ä¼¼åº¦ {score:.4f} ---\")\n        print(\"Task:\", item.get(\"task\") or item.get(\"query_text\"))\n        print(\"FOP:\", item.get(\"fop\", {}))\n        print(\"IKS_ADV:\", item.get(\"iks\", {}).get(\"adv\", []))\n\n\n# ============================================================\n# 5. ä¸»æµç¨‹ï¼ˆKaggle å¯ç›´æ¥è¿è¡Œï¼‰\n# ============================================================\nif __name__ == \"__main__\":\n    print(\"=\" * 60)\n    print(\"ğŸš€ Neural TRIZ | FAISS å‘é‡ç´¢å¼•æ„å»ºï¼ˆKaggleï¼‰\")\n    print(\"=\" * 60)\n\n    texts, metadata = load_jsonl(JSONL_PATH)\n\n    if not texts:\n        raise RuntimeError(\"âŒ æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆ task\")\n\n    if os.path.exists(INDEX_PATH) and os.path.exists(METADATA_PATH):\n        print(\"ğŸ“Œ å‘ç°å·²æœ‰ç´¢å¼•ï¼Œç›´æ¥åŠ è½½\")\n        index = faiss.read_index(INDEX_PATH)\n        metadata = load_metadata(METADATA_PATH)\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model = SentenceTransformer(\"BAAI/bge-large-zh-v1.5\", device=device)\n    else:\n        print(\"ğŸ†• æœªå‘ç°ç´¢å¼•ï¼Œå¼€å§‹æ„å»º\")\n        index, model = build_faiss_index(texts, INDEX_PATH)\n        save_metadata(metadata, METADATA_PATH)\n\n    # =========================\n    # æ£€ç´¢æµ‹è¯•ï¼ˆç¤ºä¾‹ï¼‰\n    # =========================\n    test_queries = [\n        \"é™ä½ç”µå­è®¾å¤‡çš„æ•£çƒ­æ¸©åº¦\",\n        \"å‡å°‘æœºæ¢°ç»“æ„çš„æŒ¯åŠ¨å’Œå™ªå£°\",\n        \"æé«˜å¤ªé˜³èƒ½ç”µæ± è½¬æ¢æ•ˆç‡\"\n    ]\n\n    for q in test_queries:\n        search_demo(model, index, metadata, q, top_k=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T07:31:32.006873Z","iopub.execute_input":"2026-02-03T07:31:32.007208Z","iopub.status.idle":"2026-02-03T07:53:39.939009Z","shell.execute_reply.started":"2026-02-03T07:31:32.007169Z","shell.execute_reply":"2026-02-03T07:53:39.937712Z"}},"outputs":[{"name":"stderr","text":"2026-02-03 07:31:45.661901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770103905.862114      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770103905.919253      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770103906.408751      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770103906.408798      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770103906.408801      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770103906.408804      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"============================================================\nğŸš€ Neural TRIZ | FAISS å‘é‡ç´¢å¼•æ„å»ºï¼ˆKaggleï¼‰\n============================================================\nâœ… æˆåŠŸåŠ è½½ 124,532 æ¡æœ‰æ•ˆæ ·æœ¬\nğŸ†• æœªå‘ç°ç´¢å¼•ï¼Œå¼€å§‹æ„å»º\nğŸš€ ä½¿ç”¨è®¾å¤‡: cuda\nğŸ“¥ åŠ è½½ BGE ä¸­æ–‡å‘é‡æ¨¡å‹...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1747649fe96144a6b2476de2d43a6cec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ab95c10649345b7ac6e2e46f8a907b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b78ccc0a53d47cbad1da73abc89b158"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e7033e45a1d42d98e91006c91843f13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6fab47ac9884471b38c2a4ad33d3d8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.30G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c23d45e80e4b55ad644e198fddda53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.30G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c721af80f9094f18a13e444fa952b77a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e51c7a2eb67f43818e00872f85e47f28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3ae6b9950ac4417b2df900b620a13a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab77de38a2dc44ad902520976cc3a453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20965dccd17649bfbdd28a7c4828e43e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ab824228f65401390697d7807284935"}},"metadata":{}},{"name":"stdout","text":"ğŸ”„ ç¼–ç æ–‡æœ¬å‘é‡ï¼ˆå…± 124,532 æ¡ï¼‰...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/3892 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"863e4bf7feff44289ccc17ad6f4b47d4"}},"metadata":{}},{"name":"stdout","text":"ğŸ“Š å‘é‡ç»´åº¦: 1024\nâœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± 124,532 æ¡\nğŸ’¾ ç´¢å¼•å·²ä¿å­˜è‡³: /kaggle/working/innovation_faiss.index\nğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜è‡³: /kaggle/working/innovation_metadata.json\n\nğŸ” Query: é™ä½ç”µå­è®¾å¤‡çš„æ•£çƒ­æ¸©åº¦\n\n--- Top 1 | ç›¸ä¼¼åº¦ 0.8924 ---\nTask: èƒ½å¤Ÿæé«˜ç”µå­è®¾å¤‡çš„æ•£çƒ­æ€§èƒ½\nFOP: {'function': ['å¤„ç†å™¨', 'å¤¹è§’', 'ä¸»ä½“', 'è®¾ç½®', 'è·å–', 'ç”µå­è®¾å¤‡', 'æŠ˜å ', 'æŒ‡ä»¤', 'æ£€æµ‹', 'æ”¹å˜', 'æ¨¡å—', 'æ‰§è¡Œ', 'è·ç¦»', 'æ§åˆ¶', 'æ¸©åº¦'], 'object': ['æ‰€è¿°ç”µå­è®¾å¤‡', 'ç”µå­è®¾å¤‡', 'ç¬¬äºŒæ¸©åº¦æ¨¡å—', 'ç¬¬ä¸€æ¸©åº¦æ¨¡å—', 'ç¬¬ä¸€è·å–æ¨¡å—', 'æ§åˆ¶æ¨¡å—'], 'process': ['è®¾ç½®', 'ç”µå­è®¾å¤‡', 'æ£€æµ‹', 'ç¬¬ä¸€æ¸©åº¦å€¼', 'ç¬¬äºŒæ¸©åº¦å€¼', 'è·å–', 'ç¬¬ä¸€æ¸©åº¦æ¨¡å—', 'å……ç”µçŠ¶æ€çš„æƒ…å†µä¸‹ï¼Œè·å–æ‰€è¿°ç¬¬ä¸€æ¸©åº¦æ¨¡å—æ£€æµ‹çš„ç¬¬ä¸€æ¸©åº¦', 'ç¬¬ä¸€è·å–æ¨¡å—', 'å……ç”µçŠ¶æ€', 'å……ç”µ', 'æŠ˜å ', 'ç¬¬äºŒæ¸©åº¦æ¨¡å—', 'æ¸©åº¦', 'æ§åˆ¶æ¨¡å—', 'æ§åˆ¶è£…ç½®', 'æ§åˆ¶æ–¹æ³•', 'åŒ…æ‹¬', 'è£…ç½®']}\nIKS_ADV: ['å…‹é‡Œæ–¯å¦æ£®æ•ˆåº”', 'å†·å´', 'è§å…‰', 'æ•£å°„', 'ç¬¬äºŒå£°éŸ³', 'ï¼ˆä¸‡æœ‰ï¼‰å¼•åŠ›', 'çƒ­é‡Šå…‰æ³•', 'è´ŸæŠ˜å°„ç°è±¡', 'è •å˜æ³¢(çˆ¬è¡Œæ³¢)', 'æ³•æ‹‰ç¬¬æ•ˆåº”ï¼ˆç£æ—‹è½¬ï¼‰', 'æ ¸èšå˜', 'ç”µä»‹è´¨åŠ çƒ­', 'çƒ­éœå°”æ•ˆåº”', 'çƒ­ï¼ˆåˆ†ï¼‰è§£ï¼ˆæ•£çƒ­ï¼‰', 'çº¢å¤–è¾å°„', 'å…‰è‡´å‘å…‰', 'çˆ†ç‡ƒ', 'è¾å°„å‘å…‰', 'é˜´å½±', 'å¸ƒæ‹‰æ ¼è¡å°„']\n\n--- Top 2 | ç›¸ä¼¼åº¦ 0.8273 ---\nTask: æé«˜é›†æˆç”µè·¯è®¾å¤‡çš„æ•£çƒ­æ€§\nFOP: {'function': ['è‹¥å¹²', 'åº•åº§', 'è®¾ç½®', 'å¼•çº¿', 'è½¬è½´', 'ç”µæœº', 'æµ‹é‡', 'é¡¶éƒ¨', 'æ‰‡ç‰‡', 'é€šçº¿', 'æ•£çƒ­', 'é½¿è½®', 'æ¶‚å±‚', 'ç”µå­', 'ç©ºè…”', 'å‡¹æ§½'], 'object': ['ç©ºè…”', 'åº•éƒ¨', 'å‡¹æ§½', 'ç”µæœº', 'ç¬¬äºŒè½¬è½´', 'å°è£…å±‚', 'æ•£çƒ­å—', 'æ‰‡ç‰‡', 'æ•£çƒ­å­”', 'è®¾å¤‡ä¸»ä½“', 'ç¬¬ä¸€è½¬è½´', 'ç¬¬ä¸€é½¿è½®', 'ç¬¬äºŒé½¿è½®', 'è®¾å¤‡å†…å£'], 'process': ['è®¾ç½®', 'åº•éƒ¨', 'å»¶ä¼¸', 'å¤–ä¾§', 'æ•£çƒ­å­”', 'ç¬¬ä¸€è½¬è½´', 'è´¯ç©¿', 'å†…éƒ¨', 'ç”µæœº', 'å°è£…å±‚', 'æ•£çƒ­å—', 'æ‰‡ç‰‡', 'è®¾å¤‡ä¸»ä½“', 'ç¬¬äºŒè½¬è½´', 'ç¬¬ä¸€é½¿è½®', 'é¡¶éƒ¨', 'é›†æˆç”µè·¯å°è£…è®¾å¤‡', 'ç©ºè…”', 'å‡¹æ§½', 'ç¬¬äºŒé½¿è½®', 'è®¾å¤‡å†…å£']}\nIKS_ADV: ['è½¬çŸ©ï¼ˆåŠ›çŸ©ï¼‰', 'ç¦»å­é£', 'é™„åŠ è´¨é‡', 'å‡å‹ï¼ˆé™å‹ï¼‰', 'ç”µæ°´åŠ¨åŠ›å­¦ã€ç”µæµä½“', 'è‡ªç‡ƒæ€§', 'æ ¸èšå˜', 'æœºæ¢°ä¼˜åŠ¿ï¼ˆå¢ç›Šï¼‰', 'å†²å‡»åŠ›', 'å‰ªåº”åŠ›', 'æƒ¯é‡çŸ©ï¼›è½¬åŠ¨æƒ¯é‡ï¼›æƒ¯æ€§åŠ›çŸ©', 'æ–‡ä¸˜é‡Œæ•ˆåº”', 'ç”µå­é›ªå´©å‡»ç©¿', 'æ¶²å‹æœº', 'æ’é‡', 'ä»‹ç”µå¸¸æ•°', 'é»æ»åŠ çƒ­', 'æ±¤æ£®æ”¾ç”µ', 'å‹é™', 'å‰ªåˆ‡å¢ç¨ ï¼ˆå¢å¼ºï¼‰']\n\n--- Top 3 | ç›¸ä¼¼åº¦ 0.8071 ---\nTask: æé«˜è£…ç½®çš„æ•£çƒ­èƒ½åŠ›\nFOP: {'function': ['æ¡†æ¶', 'æ¨¡å—æä¾›å®‰è£…åŒºåŸŸ', 'è®¾ç½®', 'æ€§èƒ½', 'æœ‰æœº', 'æµ‹é‡', 'ä¸ºæ¨¡å—æä¾›å®‰è£…åŒºåŸŸ', 'è¾¹å­”', 'åŒºåŸŸ', 'åˆ†ç¦»', 'å†²å‹', 'è‹¥å¹²', 'æ”¹å˜', 'åŠ å·¥', 'ç¼“å†²ä½œç”¨', 'é£æ‰‡', 'ä½ç½®', 'æœºç®±', 'é è¿‘', 'æœºæ¶'], 'object': ['æœºç®±', 'æ¡†æ¶', 'ä¸Šç›–æ¿', 'ä¸Šå±‚ç»“æ„', 'æ‰€è¿°æœºæ¶(2)', 'é”ç´§æœºæ„', 'æ¨¡å—', 'è¿›é£å£', 'ç¼“å†²ä½œç”¨', 'æœºæ¶', 'ç¼“æŒ¯æ¡', 'å®‰è£…åŒºåŸŸ', 'ä¸‹ç›–æ¿', 'ä¸‹å±‚ç»“æ„', 'å‡ºé£å£', 'ä¾§å£'], 'process': ['æ¡†æ¶', 'è®¾ç½®', 'ä¸Šå±‚ç»“æ„', 'è¿›é£å£', 'è¿æ¥', 'æ¨¡å—', 'å®‰è£…åŒºåŸŸ', 'ä¸Šç›–æ¿', 'é”ç´§æœºæ„', 'ç¼“å†²ä½œç”¨', 'ç¼“æŒ¯æ¡', 'ç´§å¯†è¿æ¥', 'å¼ºè¿«é£å†·', 'ä¸‹å±‚ç»“æ„', 'æœºç®±', 'æœºè½½ç»“æ„æ•£çƒ­è£…ç½®', 'æä¾›', 'åŒ…æ‹¬', 'æœºæ¶', 'åˆ¶æˆ', 'ä¸‹ç›–æ¿', 'å‡ºé£å£', 'ä¾§å£']}\nIKS_ADV: ['å£°è¾å°„å‹åŠ›', 'å£°ç©ºåŒ–ï¼ˆå£°æ°”èš€ï¼‰', 'æ²¸è…¾', 'å†·å´', 'æ•£å°„', 'å£°åŒ–å­¦', 'è¶…å£°æ³¢æ¯›ç»†ç®¡æ•ˆåº”', 'ç»çƒ­åŠ çƒ­', 'ç©ºåŒ–ã€æ°”ç©´', 'è‡ªç‡ƒæ€§', 'æ‘†åŠ¨ï¼›é¢¤åŠ¨ï¼›èŠ±é¸Ÿï¼›æ‰‘åŠ¨ï¼›é£˜æ‰¬', 'ç ”ç£¨', 'æ¹åŠ£çƒ­', 'è •å˜æ³¢(çˆ¬è¡Œæ³¢)', 'ç»çƒ­å†·å´', 'æ ¸èšå˜', 'å†²å‡»åŠ›', 'åŒå£°æ³¢ã€åå°„æ³¢', 'ä¹±æµï¼›æ¹æµï¼›æ°”æµ', 'è¡¨é¢å£°æ³¢']\n\nğŸ” Query: å‡å°‘æœºæ¢°ç»“æ„çš„æŒ¯åŠ¨å’Œå™ªå£°\n\n--- Top 1 | ç›¸ä¼¼åº¦ 0.8073 ---\nTask: æé«˜æœºæ¢°æ•ˆç‡ å¤§å¤§é™ä½å™ªéŸ³\nFOP: {'function': ['é£å¶è½´', 'è®¾ç½®', 'ç”µæœº', 'æµ‹é‡', 'è®¾æœ‰', 'åµŒå…¥', 'é©±åŠ¨', 'åŠ¨å¸¦', 'ä¾§é¢'], 'object': ['ä¸»åŠ¨å¸¦è½®', 'é£å¶è½´', 'é£å¶ æ§½', 'å£³ä½“', 'é£å¶æ§½', 'çš®å¸¦', 'é©±åŠ¨æœºæ„', 'é©±åŠ¨ç”µæœº', 'è½´æµé£å¶', 'é£å¶æ¶', 'è¾“å‡ºè½´', 'ä»åŠ¨å¸¦è½®', 'å£³ ä½“', 'å¸¦è½®è½´'], 'process': ['é£å¶è½´', 'é…åˆ', 'çš®å¸¦', 'é©±åŠ¨æœºæ„', 'è½´æµé£å¶', 'ä½å™ªå£°', 'é£å¶æ¶', 'ç¯å½¢', 'ä»åŠ¨å¸¦è½®', 'é©±åŠ¨æœº æ„', 'å£³ ä½“', 'æ¨ªæˆªé¢', 'ç›´å¾„', 'è¿æ¥', 'å£³ä½“', 'é£å¶æ§½', 'ä¸‰è§’å½¢', 'ä¸­å¿ƒ', 'é£å¶ æ§½', 'è®¾æœ‰', 'é©±åŠ¨ç”µæœº', 'èŠ‚èƒ½ä½å™ªå£°è½´æµé£æœº', 'å®‰è£…', 'å¸¦è½®è½´', 'ä¸»åŠ¨å¸¦è½®']}\nIKS_ADV: ['æ‹‰èƒ€ææ–™', 'è½¬çŸ©ï¼ˆåŠ›çŸ©ï¼‰', 'é™„åŠ è´¨é‡', 'è¾¾å°”æ–‡æ¼‚ç§»', 'äºšç¨³æ€', 'é€Ÿç‡æ¯”', 'å‡å‹ï¼ˆé™å‹ï¼‰', 'ç”µæ°´åŠ¨åŠ›å­¦ã€ç”µæµä½“', 'é™ç”µæ„Ÿåº”', 'é»å¼¹æ€§', 'å…‰ç”µå¯¼æ€§', 'æ¬§æ°æ•ˆåº”', 'æ°”åŠ¨å¼¹æ€§é¢¤æŒ¯', 'å‰ªåˆ‡ç¨€åŒ–ï¼ˆæˆ–å‡å¡‘æ€§ï¼‰', 'æœºæ¢°ä¼˜åŠ¿ï¼ˆå¢ç›Šï¼‰', 'å†²å‡»åŠ›', 'å‹é˜»æ•ˆåº”', 'ç”µå­é›ªå´©', 'æ‘©æ“¦ç”µæ•ˆåº”', 'æ¯›ç»†æ³¢æ•ˆåº”']\n\n--- Top 2 | ç›¸ä¼¼åº¦ 0.7857 ---\nTask: ä»¥å‡å°‘æœºæ¢°èºå£°\nFOP: {'function': ['ä¸»ä½“', 'è®¾ç½®', 'æ»šåŠ¨', 'è®¾æœ‰', 'æä¾›', 'å®šå­', 'æ¥è§¦', 'è½¬åŠ¨', 'çƒä½“', 'çªæ£±', 'è´´é™„', 'å¤–åœˆ', 'åˆ¶æˆ', 'ç”µç£åœº', 'æ¥è§¦é¢'], 'object': ['è½´æ‰¿', 'è½¬å­', 'å¤– å£³', 'è½¬è½´', 'æ‰€è¿°è½¬è½´(21)', 'è½´æ‰¿åº§', 'å‡æŒ¯å…ƒä»¶', 'å®šå­', 'è½´æ‰¿å­”', 'ç”µç£åœº', 'è½¬ å­'], 'process': ['è®¾ç½®', 'è½¬è½´', 'ä¸€å®šå­', 'å¤–å£³', 'å®šå­', 'è½¬åŠ¨', 'è½´æ‰¿', 'è½¬å­', 'è®¾æœ‰', 'ç›´æµæ— åˆ·ç”µåŠ¨æœº', 'ç©¿è¿‡', 'è½´æ‰¿å‡æŒ¯å…ƒä»¶', 'ç”µç£åœº', 'å¤– å£³', 'è½´æ‰¿åº§', 'æä¾›', 'å‡æŒ¯å…ƒä»¶', 'è½´æ‰¿å­”', 'è½¬ å­']}\nIKS_ADV: ['æ‹‰èƒ€ææ–™', 'è½¬çŸ©ï¼ˆåŠ›çŸ©ï¼‰', 'ç¦»å­é£', 'äºšç¨³æ€', 'è¾¾å°”æ–‡æ¼‚ç§»', 'é™„åŠ è´¨é‡', 'ç”µè‡´ä¼¸ç¼©', 'é€Ÿç‡æ¯”', 'å‡å‹ï¼ˆé™å‹ï¼‰', 'è¨å°¼äºšå…‹æ•ˆåº”', 'è¾å°„å‹åŠ›', 'ç”µæ°´åŠ¨åŠ›å­¦ã€ç”µæµä½“', 'ç»´æ‹‰åˆ©æ•ˆåº”', 'æ´›ä»‘å…¹åŠ›', 'é»å¼¹æ€§', 'æ°”åŠ¨å¼¹æ€§é¢¤æŒ¯', 'æœºæ¢°è‡´å‘å…‰ã€åŠ›è‡´å‘å…‰', 'å‰ªåˆ‡ç¨€åŒ–ï¼ˆæˆ–å‡å¡‘æ€§ï¼‰', 'æ ¸èšå˜', 'åˆ«è´¹å°”å¾·-å¸ƒæœ—æ•ˆåº”ï¼›å¸ƒæœ—æ•ˆåº”']\n\n--- Top 3 | ç›¸ä¼¼åº¦ 0.7801 ---\nTask: ä¿è¯æœºæ„åŠ¨ä½œå¯é ã€å‡å°‘æœºæ„åŠ¨ä½œæŒ¯åŠ¨\nFOP: {'function': ['é“°æ¥', 'æ‹è‡‚', 'æ¥äº', 'é”€è½´', 'æ¥è§¦ç‚¹', 'å‚¨èƒ½', 'å¼€å…³', 'è¿æ¥', 'ä¸»è½´', 'æµ‹é‡', 'è½¬åŠ¨', 'é©±åŠ¨', 'åˆ†ç¦»', 'ä¸Šä¾§', 'æ©¡èƒ¶', 'è®¾æœ‰', 'æ”¹å˜', 'ä¼ åŠ¨', 'æŸ”æ€§', 'è¿æ†'], 'object': [], 'process': []}\nIKS_ADV: ['æ•£å°„', 'ç¦»å­é£', 'ï¼ˆä¸‡æœ‰ï¼‰å¼•åŠ›', 'è¾å°„å‹åŠ›', 'é˜´æå‘å…‰', 'ç”µæ°´åŠ¨åŠ›å­¦ã€ç”µæµä½“', 'æ¯›ç»†ç®¡å‹åŠ›', 'ç†µçˆ†ç‚¸', 'ç”µè‡´å‘å…‰ï¼ˆåœºè‡´å‘å…‰ï¼‰', 'æœºæ¢°è‡´å‘å…‰ã€åŠ›è‡´å‘å…‰', 'æ ¸èšå˜', 'å…‰è‡´å‘å…‰', 'çˆ†ç‡ƒ', 'ç”µå¼§', 'å¼•åŠ›é€é•œ', 'åŒæŠ˜å°„', 'æ™¯æ·±', 'å‹è‡´å‘å…‰', 'ï¼ˆæ¶²ä½“ï¼‰æ¸—é€', 'å›æ—‹åŠ é€Ÿè¾å°„']\n\nğŸ” Query: æé«˜å¤ªé˜³èƒ½ç”µæ± è½¬æ¢æ•ˆç‡\n\n--- Top 1 | ç›¸ä¼¼åº¦ 0.9890 ---\nTask: æå‡å¤ªé˜³èƒ½ç”µæ± çš„è½¬æ¢æ•ˆç‡\nFOP: {'function': ['ä¿ç•™', 'å½¢æˆ', 'è¿æ¥', 'æ­£é¢', 'é€‰æ‹©æ€§', 'ç¡…ç‰‡', 'æºæ‚', 'æä¾›', 'æ·±åº¦', 'ä¸Šæ–¹', 'ç¬¬ä¸€æ¬¡æ‰©æ•£', 'åˆ¶ä½œ', 'å»é™¤', 'åˆ†ç¦»', 'æµ“åº¦', 'æš´éœ²å‡º', 'ä¸‹æ–¹'], 'object': ['æ‰€è¿°ç¡…ç‰‡', 'é€‰æ‹©æ€§å‘å°„æç»“æ„', 'å¤ªé˜³èƒ½ç”µæ± ', 'ç¡…ç‰‡', 'ç¬¬äºŒæºæ‚å±‚', 'ç¬¬ä¸€æºæ‚å±‚', 'æ‰€è¿°ç¡…ç‰‡æ­£é¢', 'é¢„è®¾æ­£é¢ç”µæåŒºåŸŸ', 'æ­£é¢ç”µæ', 'åŒºåŸŸ', 'æ‚è´¨'], 'process': ['ç¬¬äºŒæ¬¡æ‰©æ•£', 'ç¡…ç‰‡', 'æ·±åº¦', 'é¢„è®¾æ­£é¢ç”µæåŒºåŸŸ', 'å»é™¤', 'æ­£é¢ç”µæ', 'æºæ‚æµ“åº¦', 'æ‚è´¨', 'é€‰æ‹©æ€§å‘å°„æç»“æ„çš„åˆ¶ä½œæ–¹æ³•', 'é¢„è®¾æ­£é¢ç”µæ', 'å¤ªé˜³èƒ½ç”µæ± ', 'æ‰©æ•£', 'ç¬¬ä¸€æ¬¡æ‰©æ•£', 'ç¬¬äºŒæºæ‚å±‚', 'ä¿ç•™', 'æš´éœ²', 'åˆ»èš€', 'é€‰æ‹©æ€§å‘å°„æç»“æ„', 'å½¢æˆ', 'å»é™¤æ‰€è¿°ç¡…ç‰‡è¡¨é¢çš„æ‚è´¨', 'ç¬¬ä¸€æºæ‚å±‚', 'è¦†ç›–', 'åˆ¶ä½œ', 'é€‰æ‹©æ€§å‘å°„æ', 'ç«¯ç‚¹å€¼']}\nIKS_ADV: ['ç”µé»æ»æ•ˆåº”', 'äºšç¨³æ€', 'é…¶', 'å…‰ç”µå¯¼æ€§', 'é™ç”µæ„Ÿåº”', 'æ¬§æ°æ•ˆåº”', 'ç”µå­é›ªå´©', 'å‡è¡€', 'æ”¾å°„è¡°å˜ï¼›è¾å°„è¡°å˜', 'è¡¨é¢å£°æ³¢', 'ç”µå­ç¢°æ’è§£å¸', 'ç¼éš™é—´è…èš€', 'åŒ–å­¦é”®', 'ç”µå­é›ªå´©å‡»ç©¿', 'ä»‹ç”µå¸¸æ•°', 'ç”µè‡´æ¶¦æ¹¿', 'ç”µç«èŠ±', 'ç§‘å®¾è¯ºæ•ˆåº”', 'æ±¤æ£®æ”¾ç”µ', 'ç”µæ™•æ”¾ç”µ']\n\n--- Top 2 | ç›¸ä¼¼åº¦ 0.8765 ---\nTask: å¯æ˜¾è‘—æé«˜å¤ªé˜³èƒ½ç”µæ± çš„å…‰ç”µè½¬æ¢æ•ˆç‡\nFOP: {'function': ['ç§°å–', 'ç”²é†‡æº¶æ¶²', 'çƒ˜ç®±', 'æ°´æº¶æ¶²', 'åŠ å…¥', 'æ²¹é…¸', 'è½¬é€Ÿ', 'ç¯å¢ƒ', 'æº¶æ¶²', 'ç”²è‹¯', 'åŠ åˆ°', 'è¿æ¥', 'æµ‹é‡', 'åŠ çƒ­', 'å‚ç›´', 'å»ç¦»å­æ°´', 'ç¦»å¿ƒ', 'æ”¹å˜', 'ç§å­', 'ç”²é†‡'], 'object': [], 'process': []}\nIKS_ADV: ['ç”µè‡´å‘å…‰ï¼ˆåœºè‡´å‘å…‰ï¼‰', 'ç”µåŠ¨è¾‰å…‰æ”¾ç”µ', 'ç”µç«èŠ±', 'ç”µå¼§', 'é™ç”µæ”¾ç”µ', 'å…‰ç”Ÿä¼æ‰“æ•ˆåº”ï¼ˆå…‰ä¼æ•ˆåº”ï¼‰', 'ç”µå­é›ªå´©', 'åŒæ­¥è¾å°„', 'ç”µæ™•æ”¾ç”µ', 'é˜´æå‘å…‰', 'ç”µè‡´åŒ–å­¦å‘å…‰', 'ç‚½çƒ­', 'å…‰ç”µå¯¼æ€§', 'å…‰è‡´ç”µç¦»']\n\n--- Top 3 | ç›¸ä¼¼åº¦ 0.8672 ---\nTask: ä»¥æé«˜å¤ªé˜³èƒ½ç”µæ± çš„è½¬æ¢æ•ˆç‡ æé«˜å¤ªé˜³èƒ½ç»„ä»¶çš„å…‰ç”µè½¬æ¢æ•ˆç‡ æé«˜å…‰ç”µè½¬æ¢æ•ˆç‡\nFOP: {'function': ['èšå…‰é•œ', 'é€é•œ', 'ç‡', 'é€å…‰', 'æµ‹é‡', 'è®¾æœ‰', 'å¢åŠ å¤ªé˜³å…‰é€è¿‡ç‡', 'åå°„å±‚', 'ç©¿è¿‡', 'èšç„¦'], 'object': ['å¤ª é˜³èƒ½ç”µæ± ç»„ä»¶', 'èšç„¦ é€é•œ', 'ç»ç’ƒåŸºç‰‡', 'å¤ªé˜³å…‰é€è¿‡ç‡', 'å…‰çº¿', 'èšå…‰é•œç‰‡å±‚', 'å¤ªé˜³èƒ½ç”µæ± ç»„ä»¶', 'åå°„å±‚', 'é€å…‰å±‚'], 'process': ['é€å…‰', 'å¤ªé˜³èƒ½ç”µæ± ç»„ä»¶', 'åå°„å±‚', 'é€å…‰å±‚', 'ç»ç’ƒåŸºç‰‡', 'å…‰çº¿', 'å¤ªé˜³å…‰', 'å¢åŠ ', 'é€è¿‡ç‡', 'å¤ª é˜³èƒ½ç”µæ± ç»„ä»¶', 'èšç„¦ é€é•œ', 'å¤ªé˜³èƒ½ç”µæ± ç»„ä»¶ç»ç’ƒ', 'ç­‰è·ç¦»', 'è®¾æœ‰', 'ç©¿è¿‡', 'å¾®å­”ç»“æ„å±‚', 'èšå…‰é•œç‰‡å±‚', 'åŒ…æ‹¬', 'å¤ªé˜³å…‰é€è¿‡ç‡', 'åå°„']}\nIKS_ADV: ['å…‹é‡Œæ–¯å¦æ£®æ•ˆåº”', 'è¿ˆæ–¯çº³æ•ˆåº”', 'æ•£å°„', 'å…‰ç”Ÿä¼æ‰“æ•ˆåº”ï¼ˆå…‰ä¼æ•ˆåº”ï¼‰', 'è§å…‰', 'è¶…äº²æ°´æ€§', 'ï¼ˆä¸‡æœ‰ï¼‰å¼•åŠ›', 'çƒ­é‡Šå…‰æ³•', 'è´ŸæŠ˜å°„ç°è±¡', 'é˜´æå‘å…‰', 'å…‰ç”µå¯¼æ€§', 'é™ç”µæ„Ÿåº”', 'æ¬§æ°æ•ˆåº”', 'ç”µè‡´å‘å…‰ï¼ˆåœºè‡´å‘å…‰ï¼‰', 'è •å˜æ³¢(çˆ¬è¡Œæ³¢)', 'æ³•æ‹‰ç¬¬æ•ˆåº”ï¼ˆç£æ—‹è½¬ï¼‰', 'æ ¸èšå˜', 'ç”µå­é›ªå´©', 'è¡¨é¢å£°æ³¢', 'ä»‹ç”µå¸¸æ•°']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# =========================\n# é…ç½®\n# =========================\nINPUT_PATH = r\"/kaggle/input/universal-innovation-training-set/kaggle_upload/universal_innovation_dataset.jsonl\"\nOUTPUT_CSV = r\"/kaggle/working/t5_innovation_train.csv\"\n\ndata_pairs = []\n\nprint(\"ğŸ“– æ­£åœ¨æ„å»º T5 è®­ç»ƒè¯­æ–™...\")\nwith open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        try:\n            item = json.loads(line)\n            \n            # 1. æ„å»º Input (æ¨¡æ‹Ÿ TRIZ çš„çŸ›ç›¾è¾“å…¥)\n            # æ ¼å¼ï¼šProblem: [ä»»åŠ¡] | Improve: [å±æ€§] | Avoid: [è´Ÿé¢å½±å“]\n            task = item.get(\"task\", \"\")\n            \n            # æå–çŸ›ç›¾å±æ€§ (ADJ)\n            adj_info = item.get(\"adj\", {})\n            direction = adj_info.get(\"direction\", \"none\")\n            attrs = \",\".join(adj_info.get(\"attribute\", []))\n            \n            input_prompt = f\"Innovation Task: {task}\"\n            if attrs:\n                if direction == \"increase\":\n                    input_prompt += f\" | Goal: Increase {attrs}\"\n                elif direction == \"decrease\":\n                    input_prompt += f\" | Goal: Reduce {attrs}\"\n                else:\n                    input_prompt += f\" | Focus: {attrs}\"\n            \n            # 2. æ„å»º Target (çº¯ç²¹çš„åˆ›æ–°æ–¹æ³•â€œç§å­â€)\n            # æ ¼å¼ï¼šMethod: [åŠŸèƒ½åŠ¨ä½œ] using [ç§‘å­¦æ•ˆåº”]\n            fops = \",\".join(item.get(\"fop\", {}).get(\"function\", [])[:5]) # å–æ ¸å¿ƒåŠ¨ä½œ\n            advs = \",\".join(item.get(\"iks\", {}).get(\"adv\", [])[:5])       # å–æ ¸å¿ƒæ•ˆåº”\n            \n            # å¦‚æœæ²¡æœ‰æ•ˆåº”ï¼Œåªç”¨åŠ¨ä½œï¼›å¦‚æœéƒ½æœ‰ï¼Œç»„åˆä¹‹\n            if advs:\n                target_text = f\"Action: {fops} | Principle: {advs}\"\n            elif fops:\n                target_text = f\"Action: {fops}\"\n            else:\n                continue # æ²¡æœ‰æ–¹æ³•çš„æ•°æ®ä¸¢å¼ƒ\n                \n            data_pairs.append([input_prompt, target_text])\n            \n        except Exception:\n            continue\n\n# è½¬ä¸º DataFrame å¹¶ä¿å­˜\ndf = pd.DataFrame(data_pairs, columns=[\"input_text\", \"target_text\"])\nprint(f\"âœ… æ„å»ºå®Œæˆï¼Œå…± {len(df)} æ¡è®­ç»ƒæ ·æœ¬ã€‚\")\nprint(\"ç¤ºä¾‹ Input: \", df.iloc[0][\"input_text\"])\nprint(\"ç¤ºä¾‹ Target:\", df.iloc[0][\"target_text\"])\n\n# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\ntrain_df, val_df = train_test_split(df, test_size=0.05, random_state=42)\ntrain_df.to_csv(r\"/kaggle/working/t5_train.csv\", index=False)\nval_df.to_csv(r\"/kaggle/working/t5_val.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T02:39:01.406416Z","iopub.execute_input":"2026-02-04T02:39:01.406777Z","iopub.status.idle":"2026-02-04T02:39:29.554560Z","shell.execute_reply.started":"2026-02-04T02:39:01.406742Z","shell.execute_reply":"2026-02-04T02:39:29.553954Z"}},"outputs":[{"name":"stdout","text":"ğŸ“– æ­£åœ¨æ„å»º T5 è®­ç»ƒè¯­æ–™...\nâœ… æ„å»ºå®Œæˆï¼Œå…± 124283 æ¡è®­ç»ƒæ ·æœ¬ã€‚\nç¤ºä¾‹ Input:  Innovation Task: ä¸ºæ‰€è¿°åŒ¿åç”¨æˆ·ç”Ÿæˆå”¯ä¸€IDå¹¶è®°å½•è®¿é—®æ¥æº é€šè¿‡åœ¨ç”¨æˆ·é¦–æ¬¡åŒ¿åè®¿é—®ä¼ä¸šç½‘ç«™æ—¶ç”Ÿæˆå”¯ä¸€IDä»¥ä¿å­˜åç»­ç”¨æˆ·åœ¨æ¯æ¬¡æµè§ˆäº¤äº’è¿‡ç¨‹ä¸­äº§ç”Ÿçš„äº¤äº’æ•°æ®å¹¶è®°å½•è®¿é—®æ¥æº å®ç°å¯¹ä¼ä¸šç½‘ç«™æ•°æ®çš„å…¨é¢å‡†ç¡®è¿½è¸ªä¸åˆ†æ ä¸ºè¡¡é‡è¥é”€ç­–ç•¥æä¾›å‡†ç¡®çš„åˆ†ææ•°æ® ä»¥æå‡ç”¨æˆ·è½¬åŒ–ç‡ | Focus: è®¿é—®,ä¿å­˜,æ£€æµ‹,åˆ†ç»„æ ‡ç­¾,åŒ¿å,é‡‡é›†\nç¤ºä¾‹ Target: Action: ç”Ÿæˆ,è¯•ç”¨,å­˜å‚¨å™¨,è®¿é—®,è®°å½•\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================\n# Author: ChatGPT + Grok Optimized (Kaggle Stability Edition)\n# Purpose: Fine-tune T5 for Innovation Generation\n# Fixes: DataLoader Deadlock, No Output, Training Hangs\n# ============================================================\n\nimport os\nimport pandas as pd\nimport torch\nimport shutil\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    T5Tokenizer,\n    T5ForConditionalGeneration,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq,\n)\n\n# 1. å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ & æ£€æŸ¥ç¯å¢ƒ\ntorch.cuda.empty_cache()\nprint(f\"ğŸ”¥ PyTorch Version: {torch.__version__}\")\nprint(f\"ğŸ”¥ CUDA Available: {torch.cuda.is_available()}\")\n\n# ============================================================\n# 2. é…ç½®ä¸è·¯å¾„\n# ============================================================\n# âš ï¸ è¯·ç¡®è®¤ä½ çš„ csv æ–‡ä»¶ç¡®å®åœ¨è¿™ä¸ªè·¯å¾„ä¸‹\nTRAIN_PATH = \"/kaggle/working/t5_train.csv\"\nVAL_PATH   = \"/kaggle/working/t5_val.csv\"\n\n# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢ç©ºè·‘\nif not os.path.exists(TRAIN_PATH):\n    raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®: {TRAIN_PATH}ï¼Œè¯·æ£€æŸ¥ä¸Šä¸€æ­¥æ˜¯å¦æˆåŠŸç”Ÿæˆ CSVï¼\")\n\nMODEL_NAME = \"Langboat/mengzi-t5-base\"\nOUTPUT_DIR = \"/kaggle/working/t5_innovation_model\"\nFINAL_SAVE_PATH = \"/kaggle/working/final_model\"\n\n# ç¨³å¥å‚æ•°é…ç½®\nBATCH_SIZE = 12          # æ˜¾å­˜å¤Ÿå¯æ”¹ 16ï¼Œæ±‚ç¨³ç”¨ 8\nEPOCHS = 3\nLR = 2e-4\nMAX_INPUT_LEN = 128\nMAX_TARGET_LEN = 64\n\n# ============================================================\n# 3. æ•°æ®é›†ç±»\n# ============================================================\nclass InnovationT5Dataset(Dataset):\n    def __init__(self, csv_path, tokenizer):\n        # fillna(\"\") é˜²æ­¢æ•°æ®ä¸­æœ‰ç©ºå€¼å¯¼è‡´æŠ¥é”™\n        self.data = pd.read_csv(csv_path).fillna(\"\")\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # å¼ºåˆ¶è½¬å­—ç¬¦ä¸²ï¼Œé˜²æ­¢ float/int æ··å…¥\n        input_text = str(row[\"input_text\"])\n        target_text = str(row[\"target_text\"])\n\n        inputs = self.tokenizer(\n            input_text,\n            max_length=MAX_INPUT_LEN,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        with self.tokenizer.as_target_tokenizer():\n            labels = self.tokenizer(\n                target_text,\n                max_length=MAX_TARGET_LEN,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\",\n            )[\"input_ids\"]\n\n        # å¿½ç•¥ padding çš„ loss è®¡ç®—\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"labels\": labels.squeeze(),\n        }\n\n# ============================================================\n# 4. æ¨¡å‹åˆå§‹åŒ–\n# ============================================================\nprint(f\"ğŸš€ Loading Model: {MODEL_NAME}\")\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\nmodel.to(\"cuda\")\n\ntrain_dataset = InnovationT5Dataset(TRAIN_PATH, tokenizer)\nval_dataset = InnovationT5Dataset(VAL_PATH, tokenizer)\nprint(f\"ğŸ“Š Train Samples: {len(train_dataset)} | Val Samples: {len(val_dataset)}\")\n\n# ============================================================\n# 5. è®­ç»ƒå‚æ•° (é˜²æ­»é”æ ¸å¿ƒé…ç½®)\n# ============================================================\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    overwrite_output_dir=True,\n    \n    # è®­ç»ƒè¶…å‚\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    learning_rate=LR,\n    weight_decay=0.01,\n    fp16=True,  # å¼€å¯æ··åˆç²¾åº¦åŠ é€Ÿ\n    \n    # âœ… ç¨³å®šæ€§ä¿®æ”¹ 1: æŒ‰ Step ä¿å­˜å’Œè¯„ä¼°ï¼Œé¿å… Epoch ç»“æŸæ—¶çš„ IO æ´ªå³°\n    save_strategy=\"steps\",\n    save_steps=500,        # æ¯ 500 æ­¥å­˜ä¸€æ¬¡\n    eval_strategy=\"steps\", # æ–°ç‰ˆ transformers å‚æ•°\n    eval_steps=500,\n    \n    # âœ… ç¨³å®šæ€§ä¿®æ”¹ 2: é™åˆ¶ä¿å­˜æ•°é‡ï¼Œé˜²æ­¢ç£ç›˜å†™æ»¡\n    save_total_limit=2,\n    load_best_model_at_end=True, # åªæœ‰æ­£å¸¸ç»“æŸæ‰åŠ è½½æœ€ä¼˜æ¨¡å‹\n    \n    # âœ… ç¨³å®šæ€§ä¿®æ”¹ 3: å¿…é¡»è®¾ä¸º 0ï¼è§£å†³ Kaggle æ­»é”çš„å”¯ä¸€è§£\n    dataloader_num_workers=0, \n    \n    logging_steps=100,\n    report_to=\"none\", # å…³é—­ wandb å‡å°‘è”ç½‘å¹²æ‰°\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n)\n\n# ============================================================\n# 6. è®­ç»ƒæ‰§è¡Œ (å¸¦ä¿é™©ä¸)\n# ============================================================\nprint(\"\\nğŸ”¥ Starting Training with Deadlock Protection...\")\n\ntry:\n    trainer.train()\n    print(\"âœ… Training finished successfully.\")\nexcept Exception as e:\n    print(f\"\\nâš ï¸ Training interrupted due to error: {e}\")\n    print(\"âš ï¸ Attempting to save current state...\")\n\n# ============================================================\n# 7. å¼ºåˆ¶ä¿å­˜ (æ— è®ºæˆè´¥)\n# ============================================================\nprint(f\"\\nğŸ’¾ Saving final model to: {FINAL_SAVE_PATH}\")\n# ç¡®ä¿ç›®å½•å­˜åœ¨\nos.makedirs(FINAL_SAVE_PATH, exist_ok=True)\n\n# ä¿å­˜æ¨¡å‹\ntrainer.save_model(FINAL_SAVE_PATH)\ntokenizer.save_pretrained(FINAL_SAVE_PATH)\n\n# éªŒè¯ä¿å­˜ç»“æœ\nif os.path.exists(os.path.join(FINAL_SAVE_PATH, \"config.json\")):\n    print(\"âœ… Model saved successfully! You can find it in the output directory.\")\nelse:\n    print(\"âŒ Save might have failed, checking checkpoints...\")\n    if os.path.exists(OUTPUT_DIR):\n        print(f\"ğŸ“‚ Checkpoints found: {os.listdir(OUTPUT_DIR)}\")\n\n# ============================================================\n# 8. æ¨ç†æµ‹è¯• (éªŒè¯æ¨¡å‹æ˜¯å¦å˜èªæ˜äº†)\n# ============================================================\nprint(\"\\nğŸ§ª Running Inference Tests...\")\nmodel.eval()\n\ndef generate_solution(task):\n    input_text = f\"Innovation Task: {task}\"\n    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).input_ids.to(\"cuda\")\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids,\n            max_length=64,\n            num_beams=5,\n            early_stopping=True,\n            no_repeat_ngram_size=2 # é˜²æ­¢å¤è¯»æœº\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\ntest_cases = [\n    \"æé«˜å¤ªé˜³èƒ½ç”µæ± æ¿çš„è½¬æ¢æ•ˆç‡ | Goal: Increase å…‰ç”µè½¬åŒ–ç‡\",\n    \"å‡å°‘é½¿è½®ä¼ åŠ¨è¿‡ç¨‹ä¸­çš„å™ªéŸ³ | Goal: Reduce æŒ¯åŠ¨\",\n    \"è§£å†³æ‰‹æœºåœ¨é«˜è´Ÿè·ä¸‹çš„å‘çƒ­é—®é¢˜ | Goal: Reduce æ¸©åº¦\"\n]\n\nfor case in test_cases:\n    print(f\"\\nğŸ”¹ Input: {case}\")\n    print(f\"ğŸ”¸ Output: {generate_solution(case)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T02:42:08.436011Z","iopub.execute_input":"2026-02-04T02:42:08.436743Z","iopub.status.idle":"2026-02-04T06:40:40.439502Z","shell.execute_reply.started":"2026-02-04T02:42:08.436707Z","shell.execute_reply":"2026-02-04T06:40:40.438595Z"}},"outputs":[{"name":"stderr","text":"2026-02-04 02:42:17.625606: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770172937.807510      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770172937.862726      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770172938.314813      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770172938.314856      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770172938.314859      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770172938.314861      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ”¥ PyTorch Version: 2.8.0+cu126\nğŸ”¥ CUDA Available: True\nğŸš€ Loading Model: Langboat/mengzi-t5-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/725k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06299b325ca44c7fb04904a7a3296721"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9088096c1e46fca0afd5d739e7e64c"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af8fa3e950840b7ac569914cbb40d28"}},"metadata":{}},{"name":"stdout","text":"ğŸ“Š Train Samples: 118068 | Val Samples: 6215\n\nğŸ”¥ Starting Training with Deadlock Protection...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/825679522.py:136: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='29517' max='29517' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [29517/29517 3:57:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.123600</td>\n      <td>1.033239</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.006400</td>\n      <td>0.967538</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.968400</td>\n      <td>0.936144</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.940100</td>\n      <td>0.913750</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.929000</td>\n      <td>0.895926</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.896900</td>\n      <td>0.883546</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.923700</td>\n      <td>0.875228</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.895000</td>\n      <td>0.860976</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.881900</td>\n      <td>0.857711</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.860700</td>\n      <td>0.848227</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.895600</td>\n      <td>0.844555</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.881700</td>\n      <td>0.836407</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.857000</td>\n      <td>0.829088</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.833500</td>\n      <td>0.827091</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.864800</td>\n      <td>0.821332</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.837000</td>\n      <td>0.818653</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.824800</td>\n      <td>0.813933</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.833600</td>\n      <td>0.808077</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.836400</td>\n      <td>0.807070</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.744200</td>\n      <td>0.805352</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.752800</td>\n      <td>0.803352</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.786600</td>\n      <td>0.798520</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.792500</td>\n      <td>0.798042</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.769200</td>\n      <td>0.793246</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.754600</td>\n      <td>0.789383</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.748800</td>\n      <td>0.788342</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.753100</td>\n      <td>0.784048</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.764600</td>\n      <td>0.783869</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.724000</td>\n      <td>0.782343</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.755100</td>\n      <td>0.778854</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.780700</td>\n      <td>0.776268</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.743800</td>\n      <td>0.772199</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.760200</td>\n      <td>0.772159</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.764400</td>\n      <td>0.768369</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.726600</td>\n      <td>0.765538</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.728000</td>\n      <td>0.764983</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.758600</td>\n      <td>0.761980</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.737200</td>\n      <td>0.758676</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.741000</td>\n      <td>0.757230</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.668800</td>\n      <td>0.766530</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.659000</td>\n      <td>0.765462</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.646700</td>\n      <td>0.765397</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.670300</td>\n      <td>0.763458</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.667300</td>\n      <td>0.762098</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.643200</td>\n      <td>0.763711</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.657500</td>\n      <td>0.761755</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.659400</td>\n      <td>0.758877</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.673100</td>\n      <td>0.757637</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.643200</td>\n      <td>0.758089</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.670500</td>\n      <td>0.755035</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.653000</td>\n      <td>0.754571</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.645400</td>\n      <td>0.752597</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.655200</td>\n      <td>0.752515</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.670200</td>\n      <td>0.750428</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.642800</td>\n      <td>0.751428</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.653500</td>\n      <td>0.749376</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.628800</td>\n      <td>0.749293</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.644900</td>\n      <td>0.749022</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.637900</td>\n      <td>0.748802</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n","output_type":"stream"},{"name":"stdout","text":"âœ… Training finished successfully.\n\nğŸ’¾ Saving final model to: /kaggle/working/final_model\nâœ… Model saved successfully! You can find it in the output directory.\n\nğŸ§ª Running Inference Tests...\n\nğŸ”¹ Input: æé«˜å¤ªé˜³èƒ½ç”µæ± æ¿çš„è½¬æ¢æ•ˆç‡ | Goal: Increase å…‰ç”µè½¬åŒ–ç‡\nğŸ”¸ Output: Action: | Principle: å…‰ç”Ÿä¼æ‰“æ•ˆåº”(å…‰ä¼æ•ˆåº”),é˜´æå‘å…‰,å…‰ç”µå¯¼æ€§,é™ç”µæ„Ÿåº”,æ¬§æ°æ•ˆåº”\n\nğŸ”¹ Input: å‡å°‘é½¿è½®ä¼ åŠ¨è¿‡ç¨‹ä¸­çš„å™ªéŸ³ | Goal: Reduce æŒ¯åŠ¨\nğŸ”¸ Output: Action: | Principle: æ‹‰èƒ€ææ–™,è½¬çŸ©(åŠ›çŸ©),é™„åŠ è´¨é‡,è¾¾å°”æ–‡æ¼‚ç§»,é€Ÿç‡æ¯”\n\nğŸ”¹ Input: è§£å†³æ‰‹æœºåœ¨é«˜è´Ÿè·ä¸‹çš„å‘çƒ­é—®é¢˜ | Goal: Reduce æ¸©åº¦\nğŸ”¸ Output: Action: | Principle: çƒ­ç¦»å­(ç”µå­)å‘å°„,å†·å´,ç¬¬äºŒå£°éŸ³,å…‰ç”µå¯¼æ€§,é™ç”µæ„Ÿåº”\n","output_type":"stream"}],"execution_count":3}]}